{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv\n",
    "from overcooked_ai_py.agents.agent import AgentPair, RandomAgent\n",
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Define EVENT_TYPES\n",
    "EVENT_TYPES = ['tomato_pickup', 'useful_tomato_pickup', 'tomato_drop', 'useful_tomato_drop', 'potting_tomato', 'onion_pickup', 'useful_onion_pickup', 'onion_drop', 'useful_onion_drop', 'potting_onion', 'dish_pickup', 'useful_dish_pickup', 'dish_drop', 'useful_dish_drop', 'soup_pickup', 'soup_delivery', 'soup_drop', 'optimal_onion_potting', 'optimal_tomato_potting', 'viable_onion_potting', 'viable_tomato_potting', 'catastrophic_onion_potting', 'catastrophic_tomato_potting', 'useless_onion_potting', 'useless_tomato_potting']\n",
    "\n",
    "# Define available_maps_path\n",
    "available_maps_path = 'data/'\n",
    "\n",
    "# Define all_layouts\n",
    "all_layouts = ['pipeline', 'forced_coordination', 'bonus_order_test', 'simple_o', 'inverse_marshmallow_experiment', 'long_cook_time', 'simple_tomato', 'scenario1_s', 'cramped_corridor', 'random3', 'multiplayer_schelling', 'tutorial_3', 'coordination_ring', 'cramped_room_single', 'cramped_room_tomato', 'counter_circuit_o_1order', 'small_corridor', 'five_by_five', 'centre_objects', 'forced_coordination_tomato', 'counter_circuit', 'scenario3', 'old_dynamics_put_test', 'large_room', 'bottleneck', 'tutorial_1', 'schelling', 'random0', 'soup_coordination', 'cramped_room', 'scenario2_s', 'marshmallow_experiment_coordination', 'corridor', 'scenario4', 'mdp_test', 'old_dynamics_cook_test', 'cramped_room_o_3orders', 'asymmetric_advantages', 'tutorial_2', 'unident', 'schelling_s', 'you_shall_not_pass', 'tutorial_0', 'scenario2', 'centre_pots', 'simple_o_t', 'marshmallow_experiment', 'asymmetric_advantages_tomato', 'm_shaped_s']\n",
    "\n",
    "# Define included_layouts\n",
    "included_layouts = ['cramped_room', 'asymmetric_advantages_tomato', 'coordination_ring', 'forced_coordination', 'counter_circuit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = OvercookedGridworld.from_layout_name('cramped_room')\n",
    "env = OvercookedEnv.from_mdp(mdp)\n",
    "# print(layout)\n",
    "print(env)\n",
    "obs0, obs1 = env.lossless_state_encoding_mdp(env.state)\n",
    "print(obs0.shape, obs1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_obs0 = np.transpose(obs0, (2, 0, 1))\n",
    "_, w, h = trans_obs0.shape\n",
    "w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.lossless_state_encoding_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 1, 1],\n",
       "        [1, 1, 2, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 2, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 2, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 2, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [2, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[2, 1, 2, 2],\n",
       "        [2, 1, 1, 1],\n",
       "        [1, 1, 1, 2],\n",
       "        [2, 1, 1, 1],\n",
       "        [2, 1, 2, 2]],\n",
       "\n",
       "       [[1, 2, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 2, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 2],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 2],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]],\n",
       "\n",
       "       [[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_obs0 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 9, 5)\n"
     ]
    }
   ],
   "source": [
    "width_diff = (max_width - w ) // 2\n",
    "height_diff = (max_height - h) // 2\n",
    "padded_trans_obs0 = np.pad(trans_obs0 + 1, ((0, 0), (width_diff, max_width - w - width_diff), (height_diff, max_height - h - height_diff)), mode='constant', constant_values=0)\n",
    "print(padded_trans_obs0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_trans_obs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-1, 0), {'action_probs': array([0.2, 0.2, 0.2, 0.2, 0.2, 0. ])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_action, random_action_probs = agent.action(env.state)\n",
    "random_action, random_action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 1/1 [00:00<00:00, 16.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['ep_actions', 'ep_infos', 'ep_dones', 'env_params', 'ep_lengths', 'metadatas', 'ep_returns', 'ep_rewards', 'ep_states', 'mdp_params'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_pair = AgentPair(RandomAgent(), RandomAgent())\n",
    "trajectories = env.get_rollouts(action_pair, num_games=1, display_phi=True)\n",
    "trajectories.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OvercookedGridworld' object has no attribute 'lossless_state_encoding_mdp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xc/bw_4ls6j63z1jxbdlwsq5ph80000gn/T/ipykernel_21627/2541550167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossless_state_encoding_mdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'OvercookedGridworld' object has no attribute 'lossless_state_encoding_mdp'"
     ]
    }
   ],
   "source": [
    "mdp.lossless_state_encoding_mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action_probs': array([0.2, 0.2, 0.2, 0.2, 0.2, 0. ])},\n",
       " {'action_probs': array([0.2, 0.2, 0.2, 0.2, 0.2, 0. ])}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories['ep_infos'][0][0]['agent_infos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_shaping_factor = 0.1\n",
    "sparse_rewards = np.array(trajectories['ep_rewards']) # num episodes by num_steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_rewards = []\n",
    "# num episodes, num steps, num agents = 2\n",
    "for game in range(len(trajectories['ep_infos'])):\n",
    "    rewards = []\n",
    "    for d in trajectories['ep_infos'][game]:\n",
    "        potential = d['phi_s_prime'] - d['phi_s']\n",
    "        rewards.append(potential)\n",
    "    dense_rewards.append(rewards)\n",
    "dense_rewards = np.array(dense_rewards)\n",
    "print(dense_rewards.shape)\n",
    "reward1 =  sparse_rewards + dense_rewards * reward_shaping_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories['ep_infos'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories['ep_rewards'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_shaping_factor = 0.1\n",
    "sparse_rewards = np.array(trajectories['ep_rewards']) # num episodes by num_steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   5,  25, 125])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 ** np.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_rewards = []\n",
    "# num episodes, num steps, num agents = 2\n",
    "for game in range(len(trajectories['ep_infos'])):\n",
    "    rewards = []\n",
    "    for d in trajectories['ep_infos'][game]:\n",
    "        potential = d['phi_s_prime'] - d['phi_s']\n",
    "        rewards.append(potential)\n",
    "    dense_rewards.append(rewards)\n",
    "dense_rewards = np.array(dense_rewards)\n",
    "print(dense_rewards.shape)\n",
    "reward1 =  sparse_rewards + dense_rewards * reward_shaping_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_infos': [{'action_probs': array([0.2, 0.2, 0.2, 0.2, 0.2, 0. ])},\n",
       "  {'action_probs': array([0.2, 0.2, 0.2, 0.2, 0.2, 0. ])}],\n",
       " 'sparse_r_by_agent': [0, 0],\n",
       " 'shaped_r_by_agent': [0, 0],\n",
       " 'phi_s': 30.633689117862914,\n",
       " 'phi_s_prime': 30.633689117862914}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories['ep_infos'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories['ep_rewards'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_shaping_factor = 0.1\n",
    "sparse_rewards = np.array(trajectories['ep_rewards']) # num episodes by num_steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 400)\n",
      "(1, 400)\n"
     ]
    }
   ],
   "source": [
    "dense_rewards = []\n",
    "# num episodes, num steps, num agents = 2\n",
    "for game in range(len(trajectories['ep_infos'])):\n",
    "    rewards = []\n",
    "    for d in trajectories['ep_infos'][game]:\n",
    "        potential = d['phi_s_prime'] - d['phi_s']\n",
    "        rewards.append(potential)\n",
    "    dense_rewards.append(rewards)\n",
    "dense_rewards = np.array(dense_rewards)\n",
    "print(dense_rewards.shape)\n",
    "reward1 =  sparse_rewards + dense_rewards * reward_shaping_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep_actions: (1, 400)\n",
      "ep_infos: (1, 400)\n",
      "ep_dones: (1, 400)\n",
      "env_params: (1,)\n",
      "ep_lengths: (1,)\n",
      "ep_returns: (1,)\n",
      "ep_rewards: (1, 400)\n",
      "ep_states: (1, 400)\n",
      "mdp_params: (1,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for k, v in trajectories.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories['ep_returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent_evaluator = AgentEvaluator.from_mdp(mdp, env_params={\"horizon\": 400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 1/1 [00:00<00:00, 20.03it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f09660aa764c019e64f6a0c49fa15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_trajectory = agent_evaluator.evaluate_random_pair(num_games=1, display=False)\n",
    "StateVisualizer().display_rendered_trajectory(random_trajectory, ipython_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 180.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4bf55dcce2498298f9bdef0f45aafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "greedy_trajectory = agent_evaluator.evaluate_human_model_pair(num_games=1, display=False)\n",
    "StateVisualizer().display_rendered_trajectory(greedy_trajectory, ipython_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X       X       P       X       X       \n",
      "\n",
      "O       ↑0              →1      O       \n",
      "\n",
      "X                               X       \n",
      "\n",
      "X       D       X       S       X       \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "greedy_trajectory = agent_evaluator.evaluate_human_model_pair(num_games=1, display=False)\n",
    "StateVisualizer().display_rendered_trajectory(greedy_trajectory, ipython_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomAgent' object has no attribute 'joint_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent_trajectory \u001b[38;5;241m=\u001b[39m \u001b[43magent_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_agent_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m StateVisualizer()\u001b[38;5;241m.\u001b[39mdisplay_rendered_trajectory(agent_trajectory, ipython_display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/cs234/overcooked_ai/src/overcooked_ai_py/agents/benchmarking.py:273\u001b[0m, in \u001b[0;36mAgentEvaluator.evaluate_agent_pair\u001b[0;34m(self, agent_pair, num_games, game_length, start_state_fn, metadata_fn, metadata_info_fn, display, dir, display_phi, info, native_eval)\u001b[0m\n\u001b[1;32m    267\u001b[0m horizon_env\u001b[38;5;241m.\u001b[39mstart_state_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstart_state_fn\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_state_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m start_state_fn\n\u001b[1;32m    271\u001b[0m )\n\u001b[1;32m    272\u001b[0m horizon_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhorizon_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_phi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_phi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_info_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_info_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/cs234/overcooked_ai/src/overcooked_ai_py/mdp/overcooked_env.py:523\u001b[0m, in \u001b[0;36mOvercookedEnv.get_rollouts\u001b[0;34m(self, agent_pair, num_games, display, dir, final_state, display_phi, display_until, metadata_fn, metadata_info_fn, info)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m range_iterator:\n\u001b[1;32m    521\u001b[0m     agent_pair\u001b[38;5;241m.\u001b[39mset_mdp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmdp)\n\u001b[0;32m--> 523\u001b[0m     rollout_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_agents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_final_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_phi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_phi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_until\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay_until\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     (\n\u001b[1;32m    532\u001b[0m         trajectory,\n\u001b[1;32m    533\u001b[0m         time_taken,\n\u001b[1;32m    534\u001b[0m         tot_rews_sparse,\n\u001b[1;32m    535\u001b[0m         _tot_rews_shaped,\n\u001b[1;32m    536\u001b[0m     ) \u001b[38;5;241m=\u001b[39m rollout_info\n\u001b[1;32m    537\u001b[0m     obs, actions, rews, dones, infos \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    538\u001b[0m         trajectory\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    539\u001b[0m         trajectory\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m         trajectory\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m    543\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/cs234/overcooked_ai/src/overcooked_ai_py/mdp/overcooked_env.py:453\u001b[0m, in \u001b[0;36mOvercookedEnv.run_agents\u001b[0;34m(self, agent_pair, include_final_state, display, dir, display_phi, display_until)\u001b[0m\n\u001b[1;32m    450\u001b[0m s_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Getting actions and action infos (optional) for both agents\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m joint_action_and_infos \u001b[38;5;241m=\u001b[39m \u001b[43magent_pair\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoint_action\u001b[49m(s_t)\n\u001b[1;32m    454\u001b[0m a_t, a_info_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mjoint_action_and_infos)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(a \u001b[38;5;129;01min\u001b[39;00m Action\u001b[38;5;241m.\u001b[39mALL_ACTIONS \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m a_t)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomAgent' object has no attribute 'joint_action'"
     ]
    }
   ],
   "source": [
    "agent = \"something\"\n",
    "agent_trajectory = agent_evaluator.evaluate_agent_pair(agent, num_games=1, display=False)\n",
    "StateVisualizer().display_rendered_trajectory(agent_trajectory, ipython_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "overcooked_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
